---
title: "Multiple source files"
author: "Matti Vuorre"
date: "`r Sys.Date()`"
bibliography: common/bibliography.bib
csl: common/apa.csl
---

```{r chunk_opts, echo=F, warning=FALSE, message=FALSE, results = 'hide'}
source("common/chunk_opts.R")
# opts_chunk$set(cache = F)  # reset cache
# Splits the downloaded data into country-specific files
# c_list <- read_tsv("data/big5/data.csv") %>%
#     split(., .$country)
# names(c_list)
# split_write <- function(x) {
#     write.csv(c_list[[x]], 
#               file = paste0("data/big5/countries/",
#                             x, ".csv"),
#               row.names = FALSE)
# }
# sapply(names(c_list), split_write)
```

# Introduction

In this project, we'll learn to work with multiple source data files. We'll be using the same data as in the [questionnaire data project](../questionnaire/questionnaire.html), but this time begin with many files, instead of a single master spreadsheet. For beginners, I recommend first investigating the questionnaire data project.

Great info here: http://r4ds.had.co.nz/import.html

# Project organization

```bash
toolbox
|-- multisource.Rmd  # This file!
|-- data
|   |-- big5
|       |-- countries
|           |-- (nu.csv
|           |-- A1.csv
|           |-- ...
|           |-- ZW.csv
|   |-- output
```

# Accessing source files

Before we can access the data in RStudio, we need to set the working directory. Do __not__ set working directories in your script, because these calls make others with different root folder structures (usernames, systems, etc.) unable to run your script without changing these arguments. Instead, you can simply click the Set As Working Directory button in RStudio as described [here](http://mvuorre.github.io/toolbox/rrstudio/rrstudio.html#Get_started_with_RStudio).

The raw data downloaded from the [website](http://personality-testing.info/_rawdata/BIG5.zip) is a neat and simple spreadsheet of data, but for our purposes, I've split the data into separate files by country. To look at our source files, we use the `list.files()` function that simply takes a path as input, and shows what's in that path.

```{r}
file_path <- "data/big5/countries/"
list.files(file_path)
```

We would certainly like to avoid manually copying and pasting these files into one master file in Excel. Fortunately, programming allows us to [DRY](https://en.wikipedia.org/wiki/Don't_repeat_yourself), and we can load and merge these data with a few lines of code. 

# Combine multiple source files

We have a folder with a bunch of .csv files that should all be included in our main investigation. It is helpful, then, to combine the multiple files into one object that we can then wrangle in R. Additionally, we can create a shortcut for later analyses by saving this "master" data object into a "master" file. However, it is crucial that you don't just save this new master file, but also the first step in the data analytic pipeline: The merging of multiple source files. After all, it is possible that we commit an error at this stage, and if the stage, that is the R script, isn't saved, we will not know what that error was, or how it influenced the data we analysed. Therefore, I generally recommend only saving the R script that produces master data frames from raw data, not the resulting master data frame. If the merging step takes a long time (and it won't if you have fewer than a million observations), then you may also save the master data file, and load it on subsequent analyses of data instead of running the merging script every time. In any case, the merging script __must__ be saved as well. This constitutes the first note on the paper trail we leave for future investigators, including your future self.

All the country-specific source data files are identical to each other in terms of the columns they contain: _their **shapes** are identical_. This would also be the case for experimental data, and in many other cases, but sometimes raw data isn't as clean as we have it here. However, for our first experience with multiple source data files, we face the easier task of combining multiple files that have the same shape. 

## Merging two data files

The first step is to investigate what this shape actually is, so let's read and investigate a single source file, the Big Five questionnaire data from Papua New Guinea:

```{r}
library(readr)  # For enhanced data file reading functions
pg <- read_csv(paste0(file_path, "PG.csv"))
pg
```

To learn about what I called the **shape** of the data, we need to know its dimensions. These are listed on top of the displayed output, above. The object `pg` is a data frame with 2 rows and 57 columns. Each row is an observation, and each column is a variable. The easiest merging operation would be to add more rows to this data from a data frame that has the same columns, such as the data collected in Guatemala:

```{r}
( gt <- read_csv(paste0(file_path, "GT.csv")) )
```

Note how I enclosed the line of code in parentheses. By doing so, I asked R to evaluate the code _and_ print out the resulting object. Neat. 

This data frame also has 57 columns, and both data sets are two-dimensional[^1]. To merge these data sets, we simply need to bind them by row. But before we merge data frames, we need to be sure that the columns mean the same thing in both. One way to do this is to inspect the names of the columns, and make sure that they match up:

```{r}
pg_names <- names(pg)
gt_names <- names(gt)
pg_names == gt_names
```

The equality of each column name in `pg` and `gt` is `TRUE`. This is machine language for: "Stuff's the same." We can therefore proceed and bind the data frames by row. For very simple tasks, Rs built-in `rbind()` is the right tool:

```{r}
rbind(pg, gt)
```

Job done! Kind of... Although `rbind()` does its job well, we instead turn to another gem from the `dplyr` package [@wickham_dplyr:_2015]: `bind_rows()`. This function allows us to do more complex operations, such as joining _lists_ of data frames, without extra complications, and is therefore preferred:

```{r}
(pg_and_gt <- bind_rows(pg, gt))
```

Great! Although we were able to efficiently combine two source data files with this procedure, many projects, such as this one, can have hundreds of source data files, and we wouldn't want to write out `read_csv(paste0(file_path, "PG.csv"))` for each of the files separately. Therefore we'll _write a function_ that reads an arbitrary number of data files into a single object!

## Merging more than two data files

A common task, such as this one, has of course been solved many times over, and we shouldn't be wasting our time reinventing the wheel. We therefore will use a [function that another R user has already shared](https://gist.github.com/crsh/357458c41fd3d554fb24) on the excellent open-source programming platform [GitHub](https://github.com/). 

### Creating a function

What follows is based on [this function]((https://gist.github.com/crsh/357458c41fd3d554fb24)), but I've tweaked it a little bit. First, here is what the function looks like:

```{r, echo = F}
batch_read <- function(path, extension) {
  file_names <- list.files(path, pattern = extension)
  data_list <- lapply(paste0(path, file_names), read_csv)
  data_frame <- bind_rows(data_list)
  data_frame 
  }
```

```{r}
batch_read  # Call a function without () to show the methods it contains
```

Perhaps you can intuit that this function executes all the steps we already did above, but instead of merging two files, the function can merge a whole list of files! The printout above describes `batch_read()` as a _function_ that takes two arguments: `path` and `extension`. When this function is executed, it first lists files in `path` whose filename include `extension`. This is extremely helpful, because the function now only reads files that have our pre-specified extension, and ignores all other file types. This list of file names is saved to `file_names`. 

The second line is a little more complicated, but not much! (One reason it looks more complicated than it is has to do with [nesting functions vs. piping functions](../questionnaire/questionnaire.html#The_pipe)). The innermost function `paste0` concatenates multiple text strings together, without spaces (hence `paste0` instead of `paste`):

```{r}
string1 <- "Hello"
string2 <- "world!"
paste(string1, string2)
paste0(string1, string2)
```

Because `path` is a text string, and `file_names` is a list of text strings, the result is a new list of text strings. The result of `paste0(path, file_names)` is then passed to `lapply()`, which simply means _list apply_. `lapply()` takes as its first argument a list, here the list of file names. The second argument to `lapply()` is the function to apply to the list: We want to `read_csv()`. This line, therefore, _reads a list of csv files_, and saves the list into `data_list`: A list containing a whole bunch of data frames create by repeatedly applying `read_csv()` to file names in the list of names. 

A final step remains: The list needs to be unpacked into a data frame. Above, I alluded to how `bind_rows()` does all sorts of neat things. Well, this is just one of those things, we simply passed this list of data frames to `bind_rows()`, which then created a single object (called `data_frame`). The last line of the function definition specifies what is to be returned when a user calls this function.

I said before that programming is hard. You learn it by doing it. The preceding discussion is abstract and elusive and we will learn faster by implementing the function ourselves: To create this function, type the following code into your R script:

```{r}
batch_read <- function(path, extension) {
  file_names <- list.files(path, pattern = extension)
  data_list <- lapply(paste0(path, file_names), read_csv)
  data_frame <- bind_rows(data_list)
  data_frame
}
```

You can now call this function by typing in `batch_read(folder_where_revolutionary_data_lives, file_extension)`. Brilliant! Groundbreaking!

### Read multiple files with `batch_read()`

Let's put our shiny new `batch_read()` to use and load our data. Recall that `file_path` is an object that simply contains, as a text string, the folder where the country-specific data files are: `r file_path`

```{r}
d <- batch_read(file_path, ".csv")
```

There! We read all the .csv files in `file_path` (`r file_path`). If we would like to save the new master data file as a single .csv, we could run the following code (but recall the discussion above about _not saving intermediate data files, but only the *procedure* that generated those data files_):

```{r, eval = F}
write_csv(d, path = "data/big5/data.csv")
```

# Bonus round

This was a pretty boring, but important, project, so let's do something fun at the end. The country names in `d` are two letter codes ([ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2) codes, specifically): 
```{r}
c(unique(d$country)[1:20], "...")  # Print first 20 ISO2 labels and an ellipsis
```

It would be very informative if we could convert them to actual country names. Well, [there's an R package for that](https://www.youtube.com/watch?v=yhTerzNFLbo):

```{r}
library(countrycode)                                    # Install if necessary
d$country <- countrycode(sourcevar = d$country,         # Source variable
                         origin = "iso2c",              # Coding scheme
                         destination = "country.name")  # I want their names!
c(unique(d$country)[1:20], "...")  # Print again
```

And finally, we'll print out some arbitrary plot, and you should try to figure out what the code here does. If you are unsure about all the `mutate()`s etc. please refer back to the [questionnaire data project](../questionnaire/questionnaire.html).

```{r}
filter(d, !is.na(country)) %>%
    group_by(country) %>%
    mutate(N = n()) %>%
    ungroup() %>%
    filter(N > 100) %>%
    mutate(country = reorder(country, N)) %>%
    ggplot(aes(x = N, y = country)) +
    geom_point()
```

[^1]: Two-dimensional data simply means that it has rows and columns. One-dimensional data would be a vector: A simple sequence of values. By and large, we deal with two-dimensional data (such as you see when you stare at an Excel spreadsheet), but data can take any number of dimensions. Think about what three-dimensional data would look like.

# References
<br>
